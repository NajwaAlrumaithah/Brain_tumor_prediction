{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b111a387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing Brain Tumor Predictor...\n",
      "Using device: cuda\n",
      "‚úÖ Found model at: C:\\Users\\Taif\\Desktop\\brain api\\models\\reliable_model.pth\n",
      "üì• Loading brain tumor model from C:\\Users\\Taif\\Desktop\\brain api\\models\\reliable_model.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Taif\\Desktop\\brain api\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Taif\\Desktop\\brain api\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n",
      "üèÜ Original validation accuracy: 94.08%\n",
      "‚úÖ Model test passed! Output shape: torch.Size([1, 4])\n",
      "‚úÖ Predictor initialized!\n",
      "üöÄ Starting Brain Tumor Classification Web App...\n",
      "üåê Open http://localhost:8080 in your browser\n",
      "üìÅ Upload folder: static/uploads\n",
      "üìÅ Current directory: c:\\Users\\Taif\\Desktop\\brain api\n",
      "üìÅ Files in current directory: ['.venv', 'app.ipynb', 'models', 'pytorch_env', 'requirements.txt', 'static', 'templates']\n",
      "üìÅ Files in models directory: ['reliable_model.pth']\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:8080\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [26/Oct/2025 10:15:37] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Oct/2025 10:15:56] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Processing file: image15.jpg\n",
      "üéØ Prediction: pituitary, Confidence: 0.9865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [26/Oct/2025 10:15:57] \"GET /static/uploads/image15.jpg HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Oct/2025 10:16:01] \"GET /static/uploads/image15.jpg HTTP/1.1\" 304 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request, jsonify\n",
    "import os\n",
    "from werkzeug.utils import secure_filename\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.config['SECRET_KEY'] = 'brain-tumor-secret-key'\n",
    "app.config['UPLOAD_FOLDER'] = 'static/uploads'\n",
    "app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)\n",
    "\n",
    "# Allowed extensions\n",
    "ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'bmp'}\n",
    "\n",
    "class BrainTumorPredictor:\n",
    "    def __init__(self, model_path=None):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Try multiple possible model paths\n",
    "        possible_paths = [\n",
    "            r'C:\\Users\\Taif\\Desktop\\brain api\\models\\reliable_model.pth',\n",
    "            'models/reliable_model.pth',\n",
    "            './models/reliable_model.pth',\n",
    "            'reliable_model.pth',\n",
    "            '../models/reliable_model.pth'\n",
    "        ]\n",
    "        \n",
    "        # Use the first path that exists, or None if none exist\n",
    "        self.model_path = None\n",
    "        for path in possible_paths:\n",
    "            if os.path.exists(path):\n",
    "                self.model_path = path\n",
    "                print(f\"‚úÖ Found model at: {path}\")\n",
    "                break\n",
    "        \n",
    "        if not self.model_path:\n",
    "            print(\"‚ùå Model file not found in any of the expected locations\")\n",
    "            print(\"üîÑ Using demonstration mode with mock predictions\")\n",
    "                \n",
    "        self.class_names = ['glioma', 'meningioma', 'pituitary', 'notumor']\n",
    "        self.model = None\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.load_model()\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the trained model\"\"\"\n",
    "        try:\n",
    "            if self.model_path and os.path.exists(self.model_path):\n",
    "                print(f\"üì• Loading brain tumor model from {self.model_path}...\")\n",
    "                \n",
    "                # Load checkpoint\n",
    "                checkpoint = torch.load(self.model_path, map_location=self.device)\n",
    "                \n",
    "                # Create model architecture - try different approaches\n",
    "                if 'model_state_dict' in checkpoint:\n",
    "                    # Model was saved as state dict\n",
    "                    self.model = models.resnet18(pretrained=False)\n",
    "                    self.model.fc = nn.Linear(self.model.fc.in_features, 4)\n",
    "                    self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                elif 'state_dict' in checkpoint:\n",
    "                    # Alternative state dict key\n",
    "                    self.model = models.resnet18(pretrained=False)\n",
    "                    self.model.fc = nn.Linear(self.model.fc.in_features, 4)\n",
    "                    self.model.load_state_dict(checkpoint['state_dict'])\n",
    "                else:\n",
    "                    # Model was saved directly\n",
    "                    self.model = checkpoint\n",
    "                \n",
    "                self.model.to(self.device)\n",
    "                self.model.eval()\n",
    "                \n",
    "                print(\"‚úÖ Model loaded successfully!\")\n",
    "                \n",
    "                # Print model info\n",
    "                if hasattr(checkpoint, 'get'):\n",
    "                    if 'val_acc' in checkpoint:\n",
    "                        print(f\"üèÜ Original validation accuracy: {checkpoint['val_acc']:.2f}%\")\n",
    "                \n",
    "                # Test model with random input\n",
    "                self.test_model()\n",
    "                \n",
    "            else:\n",
    "                print(\"üîÑ Setting up mock model for demonstration...\")\n",
    "                self.setup_mock_model()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading model: {e}\")\n",
    "            print(\"üîÑ Setting up mock model for demonstration...\")\n",
    "            self.setup_mock_model()\n",
    "    \n",
    "    def test_model(self):\n",
    "        \"\"\"Test if model works with random input\"\"\"\n",
    "        try:\n",
    "            test_input = torch.randn(1, 3, 224, 224).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                output = self.model(test_input)\n",
    "            print(f\"‚úÖ Model test passed! Output shape: {output.shape}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Model test failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def setup_mock_model(self):\n",
    "        \"\"\"Setup mock model for demonstration\"\"\"\n",
    "        try:\n",
    "            self.model = models.resnet18(pretrained=True)\n",
    "            self.model.fc = nn.Linear(self.model.fc.in_features, 4)\n",
    "            self.model.to(self.device)\n",
    "            self.model.eval()\n",
    "            print(\"‚úÖ Mock model setup completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error setting up mock model: {e}\")\n",
    "    \n",
    "    def predict(self, image_path):\n",
    "        \"\"\"Predict brain tumor type from image\"\"\"\n",
    "        try:\n",
    "            # Load and preprocess image\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            \n",
    "            # Apply transformations\n",
    "            image_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
    "            \n",
    "            # Check if model is available\n",
    "            if self.model is None:\n",
    "                print(\"‚ö†Ô∏è Model not available, using mock prediction\")\n",
    "                return self.mock_prediction()\n",
    "            \n",
    "            # Make prediction\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(image_tensor)\n",
    "                probabilities = torch.softmax(outputs, dim=1)\n",
    "                confidence, predicted = torch.max(probabilities, 1)\n",
    "            \n",
    "            predicted_class = self.class_names[predicted.item()]\n",
    "            confidence = confidence.item()\n",
    "            all_probabilities = probabilities.cpu().numpy()[0]\n",
    "            \n",
    "            print(f\"üéØ Prediction: {predicted_class}, Confidence: {confidence:.4f}\")\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'predicted_class': predicted_class,\n",
    "                'confidence': confidence,\n",
    "                'all_probabilities': all_probabilities.tolist(),\n",
    "                'class_names': self.class_names,\n",
    "                'mock': False\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Prediction error: {e}\")\n",
    "            # Return mock prediction for demonstration\n",
    "            return self.mock_prediction()\n",
    "    \n",
    "    def mock_prediction(self):\n",
    "        \"\"\"Generate mock prediction for demonstration\"\"\"\n",
    "        try:\n",
    "            import random\n",
    "            # Create more realistic probabilities\n",
    "            base_probs = [random.uniform(0.1, 0.3) for _ in range(4)]\n",
    "            # Make one class significantly higher\n",
    "            dominant_class = random.randint(0, 3)\n",
    "            base_probs[dominant_class] += random.uniform(0.4, 0.6)\n",
    "            \n",
    "            # Normalize\n",
    "            total = sum(base_probs)\n",
    "            probabilities = [p/total for p in base_probs]\n",
    "            \n",
    "            max_idx = probabilities.index(max(probabilities))\n",
    "            predicted_class = self.class_names[max_idx]\n",
    "            confidence = probabilities[max_idx]\n",
    "            \n",
    "            print(f\"ü§ñ Mock Prediction: {predicted_class}, Confidence: {confidence:.4f}\")\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'predicted_class': predicted_class,\n",
    "                'confidence': confidence,\n",
    "                'all_probabilities': probabilities,\n",
    "                'class_names': self.class_names,\n",
    "                'mock': True  # Flag to indicate this is a mock prediction\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Mock prediction error: {e}\")\n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': 'Prediction failed'\n",
    "            }\n",
    "\n",
    "# Initialize predictor ONCE at startup\n",
    "print(\"üöÄ Initializing Brain Tumor Predictor...\")\n",
    "predictor = BrainTumorPredictor()\n",
    "print(\"‚úÖ Predictor initialized!\")\n",
    "\n",
    "def allowed_file(filename):\n",
    "    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    try:\n",
    "        if 'file' not in request.files:\n",
    "            return jsonify({'success': False, 'error': 'No file uploaded'})\n",
    "        \n",
    "        file = request.files['file']\n",
    "        \n",
    "        if file.filename == '':\n",
    "            return jsonify({'success': False, 'error': 'No file selected'})\n",
    "        \n",
    "        if file and allowed_file(file.filename):\n",
    "            # Save uploaded file\n",
    "            filename = secure_filename(file.filename)\n",
    "            filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n",
    "            file.save(filepath)\n",
    "            \n",
    "            print(f\"üìÅ Processing file: {filename}\")\n",
    "            \n",
    "            # Make prediction\n",
    "            result = predictor.predict(filepath)\n",
    "            \n",
    "            if result['success']:\n",
    "                # Fix percentage formatting if needed\n",
    "                confidence = result['confidence']\n",
    "                if confidence > 1:  # If confidence is > 1, it's probably not normalized\n",
    "                    confidence = confidence / 100\n",
    "                \n",
    "                response_data = {\n",
    "                    'success': True,\n",
    "                    'prediction': result['predicted_class'],\n",
    "                    'confidence': f\"{confidence:.2%}\",\n",
    "                    'probabilities': [\n",
    "                        {'class': cls, 'probability': f\"{prob:.2%}\"}\n",
    "                        for cls, prob in zip(result['class_names'], result['all_probabilities'])\n",
    "                    ],\n",
    "                    'image_url': f'/static/uploads/{filename}'\n",
    "                }\n",
    "                \n",
    "                if result.get('mock'):\n",
    "                    response_data['note'] = 'This is a demonstration using mock data'\n",
    "                    \n",
    "                return jsonify(response_data)\n",
    "            else:\n",
    "                return jsonify({'success': False, 'error': result.get('error', 'Prediction failed')})\n",
    "        else:\n",
    "            return jsonify({'success': False, 'error': 'Invalid file type'})\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Route error: {e}\")\n",
    "        return jsonify({'success': False, 'error': str(e)})\n",
    "\n",
    "@app.route('/test-model')\n",
    "def test_model():\n",
    "    \"\"\"Test endpoint to check model status\"\"\"\n",
    "    try:\n",
    "        test_result = predictor.test_model() if hasattr(predictor, 'test_model') else False\n",
    "        return jsonify({\n",
    "            'model_loaded': predictor.model is not None,\n",
    "            'model_path': predictor.model_path,\n",
    "            'device': str(predictor.device),\n",
    "            'test_passed': test_result\n",
    "        })\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"üöÄ Starting Brain Tumor Classification Web App...\")\n",
    "    print(\"üåê Open http://localhost:8080 in your browser\")\n",
    "    print(\"üìÅ Upload folder:\", app.config['UPLOAD_FOLDER'])\n",
    "    print(\"üìÅ Current directory:\", os.getcwd())\n",
    "    print(\"üìÅ Files in current directory:\", os.listdir('.'))\n",
    "    \n",
    "    # Check if models directory exists\n",
    "    if os.path.exists('models'):\n",
    "        print(\"üìÅ Files in models directory:\", os.listdir('models'))\n",
    "    else:\n",
    "        print(\"‚ùå Models directory not found\")\n",
    "    \n",
    "    app.run(debug=True, host='127.0.0.1', port=8080, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6985638",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
